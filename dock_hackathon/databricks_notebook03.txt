#CRIAR OS VALORES QUE SERÃƒO UTILIZADOS NOS DATAFRAMES
from pyspark.sql import *

# Create the Departments
department1 = Row(id='123456', name='Computer Science')
department2 = Row(id='789012', name='Mechanical Engineering')
department3 = Row(id='345678', name='Theater and Drama')
department4 = Row(id='901234', name='Indoor Recreation')

# Create the Employees
Employee = Row("firstName", "lastName", "email", "salary")
employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)
employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)
employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)
employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)
employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000)

# Create the DepartmentWithEmployees instances from Departments and Employees
departmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])
departmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])
departmentWithEmployees3 = Row(department=department3, employees=[employee5, employee4])
departmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])

print(department1)
print(employee2)
print(departmentWithEmployees1.employees[0].email)

-----------------------------------------------
#SEPARAR OS DADOS EM 02 DATAFRAMES

departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]
df1 = spark.createDataFrame(departmentsWithEmployeesSeq1)
display(df1)

departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]
df2 = spark.createDataFrame(departmentsWithEmployeesSeq2)
display(df2)


-------------------------------------------------
#UNIR OS 02 DATAFRAMES EM UM SÃ“

unionDF = df1.union(df2)
display(unionDF)

---------------------------------------------------
#GRAVAR A SAÃDA NO STORAGE DO DATABRICKS EM FORMATO PARQUET

dbutils.fs.rm("/tmp/databricks-df-example.parquet", True)
unionDF.write.parquet("/tmp/databricks-df-example.parquet")

---------------------------------------------------
#CARREGAR O ARQUIVO PARQUET GRAVADO ANTERIORMENTE

parquetDF = spark.read.parquet("/tmp/databricks-df-example.parquet")
display(parquetDF)

--------------------------------------------------
#INICIAR A ANÃLISE EXPLORATÃ“RIA

from pyspark.sql.functions import explode
explodeDF = unionDF.select(explode("employees").alias("e"))
flattenDF = explodeDF.selectExpr("e.firstName", "e.lastName", "e.email", "e.salary")
flattenDF.show()


---------------------------------------------------
# FILTRAR DADOS

filterDF = flattenDF.filter(flattenDF.firstName == "xiangrui").sort(flattenDF.lastName)
display(filterDF)

---------------------------------------------------
# FILTRAR DADOS

from pyspark.sql.functions import col, asc
# Use `|` instead of `or`
filterDF = flattenDF.filter((col("firstName") == "xiangrui") | (col("firstName") == "michael")).sort(asc("lastName"))
display(filterDF)

---------------------------------------------------
# FILTRAR DADOS

whereDF = flattenDF.where((col("firstName") == "xiangrui") | (col("firstName") == "michael")).sort(asc("lastName"))
display(whereDF)


---------------------------------------------------
# INICIAR DATA MUNING SUBSTITUINDO VALORES NULOS POR OUTRO VALOR QUE POSSA SER PROCESSADO

# PROCURAR NULOS
filterNonNullDF = flattenDF.filter(col("firstName").isNull() | col("lastName").isNull()).sort("email")
display(filterNonNullDF)

---------------------------------------------------
# SUBSTITUIR NULOS
nonNullDF = flattenDF.fillna("--")
display(nonNullDF)

---------------------------------------------------
# APÃ“S LIMPEZA, GERAR CONTAGENS DE VALIDAÃ‡ÃƒO OU ISOLAMENTO NO PROCESSAMENTO DE DADOS

from pyspark.sql.functions import countDistinct
countDistinctDF = nonNullDF.select("firstName", "lastName")\
  .groupBy("firstName")\
  .agg(countDistinct("lastName").alias("distinct_last_names"))
display(countDistinctDF)


----------------------------------------------------
# UTILIZAR SQL E EXPLAIN

nonNullDF.createOrReplaceTempView("databricks_df_example")

countDistinctDF_sql = spark.sql('''
  SELECT firstName, count(distinct lastName) AS distinct_last_names
  FROM databricks_df_example
  GROUP BY firstName
''')

countDistinctDF_sql.explain()

----------------------------------------------------
# SOMAR
salarySumDF = nonNullDF.agg({"salary" : "sum"})
display(salarySumDF)

----------------------------------------------------
#VERIFICAR ESTATÃSTICAS DE SALÃRIOS
type(nonNullDF.salary)
nonNullDF.describe("salary").show()

-------------------------------------------------------
# DISPLAY EM FORMATO GRÃFICO PARA ANALISE EXPLORATORIA COM MATPLOT

import pandas as pd
import matplotlib.pyplot as plt
plt.clf()
pdDF = nonNullDF.toPandas()
pdDF.plot(x='firstName', y='salary', kind='bar', rot=45)
display()